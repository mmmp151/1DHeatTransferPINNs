{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from smt.sampling_methods import LHS\n",
    "import tensorflow as tf\n",
    "import time as timelib\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as D\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "st = timelib.time()\n",
    "\n",
    "alpha = 1e-5\n",
    "xm = 7e-2\n",
    "tm = 5\n",
    "T_bl = 400.0\n",
    "T_br = 500.0\n",
    "T_init = 300.0\n",
    "\n",
    "Tm = max(T_bl, T_br, T_init)\n",
    "\n",
    "T_bl = T_bl / Tm\n",
    "T_br = T_br / Tm\n",
    "T_init = T_init / Tm\n",
    "\n",
    "# Define the domain intervals for each dimension\n",
    "xlimits = np.array([[0.0, xm], [0.0, tm]])\n",
    "\n",
    "# Create an LHS sampling instance\n",
    "sampling = LHS(xlimits=xlimits)\n",
    "\n",
    "# Generate 10000 samples\n",
    "num_samples = 10000\n",
    "samples = sampling(num_samples)\n",
    "\n",
    "x = samples[:, 0] / xm\n",
    "t = samples[:, 1] / tm\n",
    "\n",
    "x = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "t = tf.convert_to_tensor(t, dtype=tf.float32)\n",
    "x = tf.reshape(x, (-1, 1))\n",
    "t = tf.reshape(t, (-1, 1))\n",
    "\n",
    "# Define the domain intervals for each dimension\n",
    "xlimits = np.array([[0.0, xm], [0.0, 0.0]])\n",
    "\n",
    "# Create an LHS sampling instance\n",
    "sampling = LHS(xlimits=xlimits)\n",
    "\n",
    "# Generate 10000 samples\n",
    "num_samples = 10000\n",
    "samples = sampling(num_samples)\n",
    "\n",
    "x_init = samples[:, 0] / xm\n",
    "t_init = samples[:, 1] / tm\n",
    "\n",
    "x_init = tf.convert_to_tensor(x_init, dtype=tf.float32)\n",
    "t_init = tf.convert_to_tensor(t_init, dtype=tf.float32)\n",
    "x_init = tf.reshape(x_init, (-1, 1))\n",
    "t_init = tf.reshape(t_init, (-1, 1))\n",
    "\n",
    "# Define the domain intervals for each dimension\n",
    "xlimits = np.array([[0.0, 0.0], [0.0, tm]])\n",
    "\n",
    "# Create an LHS sampling instance\n",
    "sampling = LHS(xlimits=xlimits)\n",
    "\n",
    "# Generate 10000 samples\n",
    "num_samples = 10000\n",
    "samples = sampling(num_samples)\n",
    "\n",
    "x_bl = samples[:, 0] / xm\n",
    "t_bl = samples[:, 1] / tm\n",
    "\n",
    "x_bl = tf.convert_to_tensor(x_bl, dtype=tf.float32)\n",
    "t_bl = tf.convert_to_tensor(t_bl, dtype=tf.float32)\n",
    "x_bl = tf.reshape(x_bl, (-1, 1))\n",
    "t_bl = tf.reshape(t_bl, (-1, 1))\n",
    "\n",
    "# Define the domain intervals for each dimension\n",
    "xlimits = np.array([[xm, xm], [0.0, tm]])\n",
    "\n",
    "# Create an LHS sampling instance\n",
    "sampling = LHS(xlimits=xlimits)\n",
    "\n",
    "# Generate 10000 samples\n",
    "num_samples = 10000\n",
    "samples = sampling(num_samples)\n",
    "\n",
    "x_br = samples[:, 0] / xm\n",
    "t_br = samples[:, 1] / tm\n",
    "\n",
    "x_br = tf.convert_to_tensor(x_br, dtype=tf.float32)\n",
    "t_br = tf.convert_to_tensor(t_br, dtype=tf.float32)\n",
    "x_br = tf.reshape(x_br, (-1, 1))\n",
    "t_br = tf.reshape(t_br, (-1, 1))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.scatter(x.numpy(), t.numpy(), c=\"b\", s=1)\n",
    "ax.scatter(x_bl.numpy(), t_bl.numpy(), c=\"r\", s=1)\n",
    "ax.scatter(x_br.numpy(), t_br.numpy(), c=\"r\", s=1)\n",
    "ax.scatter(x_init.numpy(), t_init.numpy(), c=\"k\", s=1)\n",
    "ax.set_aspect(\"equal\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(tf.keras.Model):\n",
    "    def __init__(self, layers):\n",
    "        super(PINN, self).__init__()\n",
    "        self.losses_dict = {\"loss\": [], \"wall\": [], \"initial\": [], \"pde\": []}\n",
    "        self.layers_list = []\n",
    "        for i in range(len(layers) - 2):\n",
    "            self.layers_list.append(tf.keras.layers.Dense(layers[i+1], activation='tanh'))\n",
    "        self.layers_list.append(tf.keras.layers.Dense(layers[-1], activation='tanh'))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, t = inputs\n",
    "        X = tf.concat([x, t], axis=1)\n",
    "        for layer in self.layers_list:\n",
    "            X = layer(X)\n",
    "        return X\n",
    "\n",
    "    def PDE_loss(self, x, t):\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            tape.watch(x)\n",
    "            tape.watch(t)\n",
    "            T = self.call((x, t))\n",
    "            T_x = tape.gradient(T, x)\n",
    "        T_xx = tape.gradient(T_x, x)\n",
    "        T_t = tape.gradient(T, t)\n",
    "        del tape\n",
    "        return tf.reduce_mean(tf.square(T_t - tm * alpha / xm**2 * T_xx))\n",
    "\n",
    "    def bi_loss(self, l, r, T_bi):\n",
    "        T = self.call((l, r))\n",
    "        return tf.reduce_mean(tf.square(T - T_bi))\n",
    "\n",
    "    def plot_losses(self):\n",
    "        fig, axes = plt.subplots(1, 4, sharex=True, sharey=True, figsize=(10, 6))\n",
    "        axes[0].set_yscale(\"log\")\n",
    "        for i, j in zip(range(4), [\"loss\", \"Wall\", \"initial\", \"pde\"]):\n",
    "            axes[i].plot(self.losses_dict[j.lower()])\n",
    "            axes[i].set_title(j)\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [2, 8, 8, 8, 8, 8, 1]\n",
    "pde_solver = PINN(layers)\n",
    "\n",
    "# Build the model by making an initial call\n",
    "pde_solver((x, t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Custom training loop with Adam optimizer\n",
    "def train_adam(epochs, lr=1e-3):\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    for epoch in range(epochs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss_b = pde_solver.bi_loss(x_bl, t_bl, T_bl) + pde_solver.bi_loss(x_br, t_br, T_br)\n",
    "            loss_init = pde_solver.bi_loss(x_init, t_init, T_init)\n",
    "            loss_eq = pde_solver.PDE_loss(x, t)\n",
    "            loss = loss_b + loss_eq + loss_init\n",
    "        gradients = tape.gradient(loss, pde_solver.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, pde_solver.trainable_variables))\n",
    "\n",
    "        pde_solver.losses_dict[\"wall\"].append(loss_b.numpy())\n",
    "        pde_solver.losses_dict[\"initial\"].append(loss_init.numpy())\n",
    "        pde_solver.losses_dict[\"pde\"].append(loss_eq.numpy())\n",
    "        pde_solver.losses_dict[\"loss\"].append(loss.numpy())\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Adam Epoch {epoch}, Loss: {loss.numpy()}, Boundary Loss: {loss_b.numpy()}, Initial Loss: {loss_init.numpy()}, PDE Loss: {loss_eq.numpy()}\")\n",
    "\n",
    "# Custom training loop with L-BFGS optimizer\n",
    "def loss_fn():\n",
    "    loss_b = pde_solver.bi_loss(x_bl, t_bl, T_bl) + pde_solver.bi_loss(x_br, t_br, T_br)\n",
    "    loss_init = pde_solver.bi_loss(x_init, t_init, T_init)\n",
    "    loss_eq = pde_solver.PDE_loss(x, t)\n",
    "    loss = loss_b + loss_eq + loss_init\n",
    "    return loss\n",
    "\n",
    "def get_weights():\n",
    "    weights = [tf.reshape(var, [-1]).numpy() for var in pde_solver.trainable_variables]\n",
    "    if not weights:\n",
    "        raise ValueError(\"No trainable variables in the model. Ensure the model is properly built.\")\n",
    "    return np.concatenate(weights, axis=0)\n",
    "\n",
    "def set_weights(weights):\n",
    "    idx = 0\n",
    "    for var in pde_solver.trainable_variables:\n",
    "        var_shape = var.shape\n",
    "        var_size = tf.size(var).numpy()\n",
    "        new_value = tf.convert_to_tensor(weights[idx:idx+var_size].reshape(var_shape), dtype=tf.float32)\n",
    "        var.assign(new_value)\n",
    "        idx += var_size\n",
    "\n",
    "def lbfgs_optimizer():\n",
    "    init_params = get_weights()\n",
    "\n",
    "    def loss_and_grads(params):\n",
    "        set_weights(params)\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = loss_fn()\n",
    "        grads = tape.gradient(loss, pde_solver.trainable_variables)\n",
    "        grads = np.concatenate([tf.reshape(grad, [-1]).numpy() for grad in grads], axis=0)\n",
    "        return loss.numpy().astype(np.float64), grads.astype(np.float64)\n",
    "\n",
    "    result = minimize(loss_and_grads, init_params, jac=True, method='L-BFGS-B', options={'maxiter': 2000})\n",
    "    set_weights(result.x)\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using Adam optimizer\n",
    "train_adam(epochs=2000, lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using L-BFGS optimizer\n",
    "result = lbfgs_optimizer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pde_solver.plot_losses()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculation of numerical solution for 3s\n",
    "N_x = 600\n",
    "dx = 7e-2 / N_x\n",
    "dt = 0.5 * dx**2 / alpha\n",
    "N_t = int(3.0 / dt)\n",
    "T_prime = [[T_init * Tm] * N_x] * N_t\n",
    "T_N = np.array(T_prime, dtype=np.float32)\n",
    "r = alpha * dt / dx**2\n",
    "T_N[:, 0] = T_bl * Tm\n",
    "T_N[:, -1] = T_br * Tm\n",
    "print(N_t)\n",
    "for t_a in range(1, N_t):\n",
    "    for x_a in range(1, N_x - 1):\n",
    "        T_N[t_a, x_a] = T_N[t_a - 1, x_a] + r * (\n",
    "            T_N[t_a - 1, x_a + 1] - 2 * T_N[t_a - 1, x_a] + T_N[t_a - 1, x_a - 1]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Numerical solution at t = 3s :\", T_N[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the network output with numerical solution\n",
    "a = 0.6\n",
    "l = tf.linspace(0.0, 1.0, 600)[:, tf.newaxis]\n",
    "r = tf.ones_like(l) * a\n",
    "y = pde_solver.call((l, r)) * Tm\n",
    "print(\"Predicted temp at t = 3s\", y)\n",
    "\n",
    "error = abs((y.numpy() - T_N[-1]) / (T_N[-1]))\n",
    "print(np.max(error * 100))\n",
    "error = D(error)\n",
    "\n",
    "with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n",
    "    print(error * 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0.5\n",
    "l = tf.linspace(0.0, 1.0, 1000)[:, tf.newaxis]\n",
    "r = tf.ones_like(l) * a\n",
    "y = pde_solver.call((l, r)) * Tm\n",
    "plt.plot(l.numpy() * xm, y.numpy())\n",
    "plt.title(f\"temp distribution at t = {a * tm}\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
