{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from smt.sampling_methods import LHS\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time as timelib\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = timelib.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-5\n",
    "xm = 7e-2\n",
    "tm = 5\n",
    "T_bl = 400.0\n",
    "T_br = 500.0\n",
    "T_init = 300.0\n",
    "\n",
    "Tm = max(T_bl,T_br,T_init)\n",
    "\n",
    "T_bl = T_bl / Tm\n",
    "T_br = T_br / Tm\n",
    "T_init = T_init/ Tm\n",
    "\n",
    "# Define the domain intervals for each dimension\n",
    "xlimits = np.array([[0.0, xm],[0.0,tm]])\n",
    "\n",
    "# Create an LHS sampling instance\n",
    "sampling = LHS(xlimits=xlimits)\n",
    "\n",
    "# Generate 10000 samples\n",
    "num_samples = 10000\n",
    "samples = sampling(num_samples)\n",
    "\n",
    "\n",
    "# x = np.ones_like(t) * 0.0\n",
    "x = samples[:, 0]/ xm\n",
    "t = samples[:, 1]/ tm\n",
    "x = torch.from_numpy(x) \n",
    "t = torch.from_numpy(t) \n",
    "x = x.float().to(device)\n",
    "t = t.float().to(device)\n",
    "x.requires_grad = True\n",
    "t.requires_grad = True\n",
    "\n",
    "# Define the domain intervals for each dimension\n",
    "xlimits = np.array([[0.0, xm],[0.0,0.0]])\n",
    "\n",
    "# Create an LHS sampling instance\n",
    "sampling = LHS(xlimits=xlimits)\n",
    "\n",
    "# Generate 10000 samples\n",
    "num_samples = 10000\n",
    "samples = sampling(num_samples)\n",
    "\n",
    "x_init = samples[:, 0]/ xm\n",
    "t_init = samples[:, 1]/ tm\n",
    "x_init = torch.from_numpy(x_init) \n",
    "t_init = torch.from_numpy(t_init) \n",
    "x_init = x_init.float().to(device)\n",
    "t_init = t_init.float().to(device)\n",
    "x_init.requires_grad = True\n",
    "t_init.requires_grad = True\n",
    "\n",
    "# Define the domain intervals for each dimension\n",
    "xlimits = np.array([[0.0, 0.0],[0.0,tm]])\n",
    "\n",
    "# Create an LHS sampling instance\n",
    "sampling = LHS(xlimits=xlimits)\n",
    "\n",
    "# Generate 10000 samples\n",
    "num_samples = 10000\n",
    "samples = sampling(num_samples)\n",
    "\n",
    "x_bl = samples[:, 0]/ xm\n",
    "t_bl = samples[:, 1]/ tm\n",
    "x_bl = torch.from_numpy(x_bl) \n",
    "t_bl = torch.from_numpy(t_bl) \n",
    "x_bl = x_bl.float().to(device)\n",
    "t_bl = t_bl.float().to(device)\n",
    "x_bl.requires_grad = True\n",
    "t_bl.requires_grad = True\n",
    "\n",
    "# Define the domain intervals for each dimension\n",
    "xlimits = np.array([[xm, xm],[0.0,tm]])\n",
    "\n",
    "# Create an LHS sampling instance\n",
    "sampling = LHS(xlimits=xlimits)\n",
    "\n",
    "# Generate 10000 samples\n",
    "num_samples = 10000\n",
    "samples = sampling(num_samples)\n",
    "\n",
    "x_br = samples[:, 0]/ xm\n",
    "t_br = samples[:, 1]/ tm\n",
    "x_br = torch.from_numpy(x_br) \n",
    "t_br = torch.from_numpy(t_br) \n",
    "x_br = x_br.float().to(device)\n",
    "t_br = t_br.float().to(device)\n",
    "x_br.requires_grad = True\n",
    "t_br.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "print(len(x.cpu().detach().numpy()))\n",
    "ax.scatter(x.cpu().detach().numpy(), t.cpu().detach().numpy(), c='b', s=1)\n",
    "ax.scatter(x_bl.cpu().detach().numpy(), t_bl.cpu().detach().numpy(), c='r', s=1)\n",
    "ax.scatter(x_br.cpu().detach().numpy(), t_br.cpu().detach().numpy(), c='r', s=1)\n",
    "ax.scatter(x_init.cpu().detach().numpy(), t_init.cpu().detach().numpy(), c='k', s=1)\n",
    "ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(nn.Module):\n",
    "    def __init__(self,layers) -> None:\n",
    "        super(PINN,self).__init__()\n",
    "\n",
    "        self.losses = {\"loss\": [], \"wall\": [], \"initial\":[], \"pde\": []}\n",
    "        self.dropout_prob = 1e-10\n",
    "\n",
    "        self.layers = layers\n",
    "        self.net = nn.Sequential()\n",
    "        for i in range(len(layers) - 2):\n",
    "            self.net.add_module(f'layer_{i}', nn.Linear(layers[i], layers[i+1]))\n",
    "            self.net.add_module(f'activation_{i}', nn.Tanh())\n",
    "            # self.net.add_module(f'dropout_{i}', nn.Dropout(p=self.dropout_prob))  # Add dropout\n",
    "        self.net.add_module('output', nn.Linear(layers[-2], layers[-1]))\n",
    "        self.net.add_module(f'activation_output', nn.Tanh())\n",
    "\n",
    "        self.adam = torch.optim.Adam(self.net.parameters(),lr=5e-4 ,weight_decay=1e-4)\n",
    "        self.lbfgs = torch.optim.LBFGS(\n",
    "                                        self.net.parameters(),\n",
    "                                        lr=1,\n",
    "                                        max_iter=2000,\n",
    "                                        max_eval=2000,\n",
    "                                        tolerance_grad=0,\n",
    "                                        tolerance_change=0,\n",
    "                                        history_size=500,\n",
    "                                        line_search_fn=\"strong_wolfe\",\n",
    "                                       )\n",
    "    def forward(self,x,t):\n",
    "        X = torch.cat([x.view(-1,1),t.view(-1,1)], axis = 1)\n",
    "        u = self.net(X)\n",
    "        return u[:, 0]\n",
    "    \n",
    "    def PDE_loss(self):\n",
    "        T = self.forward(x,t)\n",
    "        T_x = torch.autograd.grad(T.sum(),x , create_graph=True)[0]\n",
    "        T_xx = torch.autograd.grad(T_x.sum(),x, create_graph=True)[0]\n",
    "        T_t = torch.autograd.grad(T.sum(),t, create_graph=True)[0]\n",
    "        return torch.mean(torch.square(T_t - tm*alpha/xm**2 * T_xx))\n",
    "    \n",
    "    def bi_loss(self , l , r , T_bi):\n",
    "        T = self.forward(l,r)       \n",
    "        return torch.mean(torch.square(T-T_bi)) \n",
    "    \n",
    "    def closure(self):\n",
    "        self.adam.zero_grad()\n",
    "        self.lbfgs.zero_grad()\n",
    "\n",
    "        loss_b = self.bi_loss(x_bl,t_bl,T_bl) + self.bi_loss(x_br,t_br,T_br)\n",
    "        loss_init = self.bi_loss(x_init,t_init,T_init)\n",
    "        loss_eq = self.PDE_loss()\n",
    "\n",
    "        self.losses['wall'].append(loss_b.detach().cpu().item())\n",
    "        self.losses['initial'].append(loss_init.detach().cpu().item())\n",
    "        self.losses['pde'].append(loss_eq.detach().cpu().item())\n",
    "\n",
    "        loss = loss_b + loss_eq + loss_init\n",
    "        self.losses['loss'].append(loss.detach().cpu().item())\n",
    "\n",
    "        print(f\"\\r epoch {len(self.losses['pde'])} , loss : {loss.detach().cpu().item():5e} , loss boundary : {loss_b.detach().cpu().item():5e} , loss initial : {loss_init.detach().cpu().item():5e} , loss PDE : {loss_eq.detach().cpu().item():5e} , time : {timelib.time()-st:.2f} s\",end=\"\",)\n",
    "        if len(self.losses['pde'])%100 ==0:\n",
    "            print(\"\")\n",
    "\n",
    "        loss.backward(retain_graph=True)\n",
    "        torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=1)\n",
    "\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def train(self,optimizer,epoch):\n",
    "        try:\n",
    "            c = 0\n",
    "            for i in optimizer:\n",
    "                if i == self.adam:\n",
    "                    print(\"\")\n",
    "                    print(\"\\noptimizer : ADAM\")\n",
    "                else:\n",
    "                    print(\"\")\n",
    "                    print(\"\\noptimizer : LBFGS\")\n",
    "                for j in range(epoch[c]):\n",
    "                    if i == self.adam:\n",
    "                        ls = self.closure()\n",
    "                        i.step()\n",
    "                    else:\n",
    "                        i.step(self.closure)\n",
    "                \n",
    "                c+=1\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\")\n",
    "            print(\"intrrupted by user\")\n",
    "        \n",
    "    def plot(self):\n",
    "        with torch.no_grad():\n",
    "            fig, axes = plt.subplots(1, 4, sharex=True, sharey=True, figsize=(10, 6))\n",
    "            axes[0].set_yscale(\"log\")\n",
    "            for i, j in zip(range(4), ['loss', 'Wall', 'initial', 'pde']):\n",
    "                axes[i].plot(self.losses[j.lower()])\n",
    "                axes[i].set_title(j)\n",
    "            plt.ylabel('Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pde_solver = PINN([2,8,8,8,8,8,1]).to(device)\n",
    "optimizer = [pde_solver.adam,pde_solver.lbfgs]\n",
    "epoch = [2000,200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pde_solver.train(optimizer,epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pde_solver.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0.6\n",
    "print(a*tm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_x = 600\n",
    "dx = 7e-2 / N_x\n",
    "dt = 0.5 * dx ** 2 / alpha\n",
    "N_t = int(5.0 / dt)\n",
    "T_prime = [[T_init*Tm]*N_x]*N_t\n",
    "T_N = np.array(T_prime,dtype=np.float32)\n",
    "r = alpha * dt / dx ** 2\n",
    "T_N[:, 0] = T_bl*Tm\n",
    "T_N[:, -1] = T_br*Tm\n",
    "print(N_t)\n",
    "for t_a in range(1,N_t):\n",
    "    for x_a in range(1,N_x-1):\n",
    "        T_N[t_a, x_a] = T_N[t_a-1, x_a] + r * (T_N[t_a-1, x_a + 1] - 2 * T_N[t_a-1, x_a] + T_N[t_a-1, x_a-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(T_N[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0.6\n",
    "a = 1.0\n",
    "l = torch.linspace(0,1,600).to(device)\n",
    "r = torch.ones_like(l)* a\n",
    "y = pde_solver(l,r)*Tm\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = abs((y.detach().cpu().numpy()-T_N[-1])/(T_N[-1]))\n",
    "print(max(error*100))\n",
    "error = D(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(error*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0.1\n",
    "l = torch.linspace(0,1,100).to(device)\n",
    "r = torch.ones_like(l)* a\n",
    "y = pde_solver(l,r)*Tm\n",
    "# l = torch.linspace(0,a*xm,1000).to(device)\n",
    "# y_ex = exact_sol(l)\n",
    "plt.plot(l.detach().cpu().numpy()*xm,y.detach().cpu().numpy())\n",
    "# plt.plot(l.detach().cpu().numpy(),y_ex.detach().cpu().numpy())\n",
    "plt.title(f\"temp distribution at t = {a*tm}\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
